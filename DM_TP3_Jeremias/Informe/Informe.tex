\documentclass {article} 

\usepackage{lmodern}
\usepackage [spanish] {babel} 
\usepackage [T1]{fontenc}
\usepackage [latin1]{inputenc}
\usepackage{amsthm} % para poder usar newtheorem
\usepackage{cancel} %Para poder hacer el simbolo "no es consecuencia semántica" etc.
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath} %para poder usar mathbb
\usepackage{amsfonts} %sigo intentando usar mathbb
\usepackage{amssymb} %therefore
\usepackage{ mathabx } %comillas
\usepackage{ verbatim } 
\theoremstyle{remark}
\newtheorem{thm}{Teorema}
\newtheorem{lem}{Lema}[section]
\newtheorem{cor}{Corolario}[section]
\newtheorem{deff}{Definición}[section]
\newtheorem{obs}{Observación}[section]
\newtheorem{ej}{Ejercicio}[section]
\newtheorem{ex}{Ejemplo}[section]
\newtheorem{alg}{Algoritmo}[section]
\usepackage[latin1]{inputenc} 
\usepackage{listings}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{proof}
\usepackage{anysize}
\marginsize{3cm}{3cm}{3cm}{3cm}
\usepackage{tikz}
\usepackage{float}


\begin{document} 



\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\Large Facultad de Ciencias Exactas, Ingeniería y Agrimensura}\\[1.5cm] % Name of your university/college

\textsc{ \Large Tópicos de Minería de Datos}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Trabajo Práctico 3: Clustering}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

%\begin{minipage}{0.4\textwidth}
%\begin{flushleft} \large
%\emph{Autor }\\
%Rodríguez Jeremías % Your name
%\end{flushleft}
%\end{minipage}

%\begin{minipage}{0.4\textwidth}
%\begin{flushright} \large
%\emph{Profesor } \\
%Mauro Jaskelioff % Supervisor's Name
%\end{flushright}
%\end{minipage}\\[4cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
\Large \emph{Alumno: Jeremías Rodríguez }\\
\Large \emph{Profesor: Pablo Granitto}\\

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\section{Ejercicio 1}
\subsection{Apartado A: Dataset Crabs}
\par Analizaré el dataset crabs, que describe cangrejos de la especie Leptograpsus Variegatus, y fue recolectado en  Fremantle, W. Australia. Consta de 200 filas con 5 features numéricas y 2 clases (sexo y especie):

\begin{verbatim}
> summary(crabs)
 sp      sex            FL              RW              CL              CW       (..)       
 B:100   F:100   Min.   : 7.20   Min.   : 6.50   Min.   :14.70   Min.   :17.10     
 O:100   M:100   1st Qu.:12.90   1st Qu.:11.00   1st Qu.:27.27   1st Qu.:31.50     
                 Median :15.55   Median :12.80   Median :32.10   Median :36.80   
                 Mean   :15.58   Mean   :12.74   Mean   :32.11   Mean   :36.41   
                 3rd Qu.:18.05   3rd Qu.:14.30   3rd Qu.:37.23   3rd Qu.:42.00   
                 Max.   :23.10   Max.   :20.20   Max.   :47.60   Max.   :54.60    
                 
\end{verbatim}
\par El objetivo es ver si, utilizando distintos pre-procesamientos en el dataset (escalado, PCA, logaritmos), los métodos de clustering estudiados (K-means y Hclust) logran agrupar por sexo o genero.
\par A continuación se muestran las 5 variantes del dataset crabs usadas. Los colores corresponden a la clase especie.


\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Crabs1.pdf} 
        \caption{Dataset sin modificaciones} \label{fig:timing1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{crabs3.pdf} 
        \caption{Transformación logarítmica + escalado} \label{fig:timing2}
    \end{subfigure}

\end{figure}

\begin{figure}
        \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{crabs4.pdf} 
        \caption{Transformación logarítmica + escalado + PCA} \label{fig:timing1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{crabs5.pdf} 
        \caption{Transformación logarítmica + PCA} \label{fig:timing2}
    \end{subfigure}



    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{crabs6.pdf} 
        \caption{Transformación logarítmica + PCA + escalado} \label{fig:timing1}
    \end{subfigure}
    \end{figure}

\newpage
\par A continuación se encuentran los resultados, los porcentajes representan los casos macheados en pares.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                    & K-means        & Hclust single & Hclust complete & Hclust average  \\  \hline \hline 
Raw                 &  57.5         &  50.5         & 59.5          &       61.5     \\ \hline
Log + scale         &  60.5         &  50.5         & 60.5          &      50.5      \\ \hline
Log + scale + PCA   &  60.5         &  50.5         & 60.5          &      50.5     \\ \hline
Log + PCA           &  60.5         &  50.5         & 60.5          &      57         \\ \hline
Log + PCA + Scale   &  100          &  50.5         & 53.5          &      50.5       \\ \hline
\end{tabular}
\caption{Feature especie: Porcentaje de casos macheados en pares, método vs dataset}
\end{center}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                    & K-means        & Hclust single & Hclust complete & Hclust average  \\  \hline \hline 
Raw                 &  50.5        &  50.5         & 50.5          &       55.5     \\ \hline
Log + scale         &  51.5        &  50.5         & 51            &      50.5      \\ \hline
Log + scale + PCA   &  51.5        &  50.5         & 51            &      50.5     \\ \hline
Log + PCA           &  51.5        &  50.5         & 51            &      51         \\ \hline
Log + PCA + Scale   &  50          &  50.5         & 56.5          &      50.5       \\ \hline
\end{tabular}
\caption{Feature sexo: Porcentaje de casos macheados en pares, método vs dataset}
\end{center}
\end{table}

\par Como vemos, casi todos los métodos fallan en encontrar alguna clusterizacion coherente con nuestras clases. En particular, ningún método permitió recuperar la división por por sexo (y los resultados fueron todos muy malos). En muchos casos se observó que en los clusters resultantes, o bien un cluster tenía casi todos los puntos del dataset; o bien ambos clusters tenían casi la misma proporción de puntos de cada clase. Incluso en algunos métodos se ven mejores resultados sin preprocesar el dataset.
\par Por otro lado, respecto a recuperar la especie de los cangrejos, los porcentajes son en general mayores aunque la gran mayoría parecieran casi aleatorios. Sin embargo, vemos que k-means con el preprocesamiento adecuado logra clusterizar perfectamente las clases. Esto resalta la importancia de hacer el preprocesamiento adecuado antes de clusterizar, que es la diferencia entre obtener basura y obtener algo muy útil. Un motivo por el que k-means puede haber funcionado es que no hay jerarquías aparentes de datos, sino que estamos buscando una partición de ellos. En varios de los plots se ven dos cúmulos de puntos bastante diferenciados, por lo que la tendencia de k-means de buscar clusters fuertemente unidos también es conveniente al problema. Esto puede explicar porqué hclust con complete linkage arroja resultados ligeramente mejores que average y single, puesto que complete linkage también apunta a clusters compactos.
\newpage
\subsection{Apartado B: Dataset Lampone}

\par El dataset Lampone consta de 49 filas y 144 features referentes a arándanos, en el cuál se intentarán recuperar las clases Año de Medición y Especie de Arándano usando las mismas técnicas de clustering. Antes de poder aplicarle los métodos, se descartaron columnas no numéricas, y se usarán los mismos preprocesamientos que en el apartado A. Dada la cantidad de features, a continuación se muestran sólo algunos ploteos ilustrativos:


\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{lampAnio.pdf} 
        \caption{Raw dataset. Primeras 10 componentes. Año de medición.} \label{fig:timing1}
    \end{subfigure}
 \end{figure}
 
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plamp2Anio.pdf} 
        \caption{log + PCA. Diez componentes más importantes. Año de medición} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plamp2Anio2.pdf} 
        \caption{log + PCA. Componentes 10-15. Año de medición} \label{fig:timing2}
    \end{subfigure}
\end{figure}

\begin{figure}

    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[width=\linewidth]{lampSpec.pdf} 
        \caption{PCA. Clases más importantes. Clase sp.} \label{fig:timing2}
    \end{subfigure}
\end{figure}

\newpage
Los resultados obtenidos fueron:

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                    & K-means         & Hclust single & Hclust complete & Hclust average \\  \hline \hline 
Raw                 &  69.39          &  57.14         & 57.14          &      57.14     \\ \hline
Log + scale         &  97.96          &  59.18         & 83.67          &      59.18     \\ \hline
Log + scale + PCA   &  97.96          &  59.18         & 83.67          &      59.18     \\ \hline
Log + PCA           &  100            &  59.18         & 85.71          &      95.92     \\ \hline
Log + PCA + Scale   &  59.18          &  61.22         & 61.22          &      62.22     \\ \hline
\end{tabular}
\caption{Feature año de medición: Porcentaje de casos macheados en pares, método vs dataset}
\end{center}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                    & K-means         & Hclust single & Hclust complete & Hclust average \\  \hline \hline 
Raw                 &  65.31          &  59.18         & 59.18          &      59.18     \\ \hline
Log + scale         &  55.1          &  57.14         & 69.39          &      57.14     \\ \hline
Log + scale + PCA   &  55.1          &  57.14         & 69.39          &      57.14     \\ \hline
Log + PCA           &  53.06            &  57.14       & 53.06          &      57.14     \\ \hline
Log + PCA + Scale   &  57.14          &  59.18         & 55.1          &      59.18      \\ \hline
\end{tabular}
\caption{Feature especie de blueberry: Porcentaje de casos macheados en pares, método vs dataset}
\end{center}
\end{table}


\par En primer lugar, ningún método logró dar un resultado aceptable recuperando la especie de arándano. Tal vez son necesarios más puntos (el dataset es muy pequeño) u otro tipo de features. Vemos en uno de los ploteos de PCA que los puntos de las dos especies están muy mezclados y no se ven dos clusters facilmente. Esto explicaría porqué todos los métodos fallan.
\par Por otro lado, el año de medición sí es recuperado exitosamente. K-means consigue separar perfectamente incluso, y hclust complete y average arrojan también buenos resultados. Vemos en los ploteos anteriores que la primer componente principal parece aportar la mayor parte de información para la separación. Mirando esta componente contra todas las otras, se pueden apreciar dos clusters con centros separados y puntos concentrados a su alrededor (a diferencia de lo que sucedía con la especie); esto favorece a k-means y a hclust complete linkage. Sin embargo, al no ser completamente compactos y diferenciados los clusters sino un poco más dispersos, se explicaría porqué average linkage tuvo mejor resultado que complete linkage bajo ciertos preprocesamientos.


\section{Ejercicio 2}
Las implementaciones se encuentran entre los archivos adjuntos. Algunas aclaraciones:

Gap Statistic:
\begin{itemize}
\item Mi implementación de GapStatistic está parametrizada por el método de clustering. Este metodo parámetro debe tomar como parámetro un dataset y un K, y devolver la suma total de las distancias $w_k$ de todos los clusters. En particular para k-means, es símplemente realizar el clustering y extraer el campo tot.withinss; mientras que en hclust debí calcular a mano las distancias entre los puntos de un mismo cluster y luego sumarlas.
\item Para calcular el gap, consideré la PCA del dataset argumento y generé puntos uniformes dentro de ella; a los que luego les apliqué el método de clustering. El algoritmo no vuelve a girar los puntos a los ejes originales, por lo que puede no funcionar correctamente para métodos que utilicen algo más que las distancias entre los puntos.
\end{itemize}

Estabilidad:
\begin{itemize}
\item El algoritmo recibe como parámetros un dataset, un entero K que indica el máximo k a analizar, y un enero B que indica cuántos datasets perturbados se generarán. Una vez generados los B datsets, se procede a comparar todas las parejas posibles y calcular el score correspondiente. Adicionalmente un parámetro proporción puede especificarse, pues el método de perturbación es subsampleo y podría desearse tomar distintas proporciones del dataset original. Finalmente, el método de clustering también es un parámetro.
\item El algoritmo retorna una matrix de tamaño $K \times \binom{B}{2}$, con los scores correspondientes a cada pareja de los B conjuntos perturbados. El algoritmo no realiza análisis alguno sobre qué k elegir.
\end{itemize}

\section{Ejercicio 3}

Se corrieron los dos algoritmos del Ejercicio 2 sobre los datasets Iris, Lampone y 4-Gaussianas. 
\subsection{Gap Statistic}
A continuación la moda\footnote{En algunos casos, el segundo valor más frecuente es indicado pués la relación de aparición es de 6 a 4} de 10 ejecuciones con cada método:

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                          & Kmeans & HclS  & HclA  & HclC \\ \hline \hline 
Iris                      &  4     &  2  &   3  &  3           \\ \hline
Iris + log + scale        &  2     &  2  &   2  &  3          \\ \hline
Lampone                   &  1     &  1  &   1  &  1          \\ \hline
Lampone + log + scale     &  3     &  1  &   1  &  2           \\ \hline
4 Gaussianas              &  4     &  1  &   2(4) &  2(4)    \\ \hline

\end{tabular}
\caption{Número óptimo de clusters para cada problema según el metodo GapStatistic.}
\end{center}
\end{table}
\par Se ve que k-means con los preprocesamientos adecuados a los conjuntos, indica el k óptimo correctamente en todos los casos. Se notó una gran fluctuación en los resultados para todos los métodos. \\
\par HCLust también retorna valores acertados, excepto en el problema de las 4 gaussianas (donde los resultados variaban mucho). En ambos casos el método funciona bastante rápido, y su costo es mucho menor al de estabilidad.

\subsection{Estabilidad}
\par Consideremos el dataset Iris. El algoritmo de estabilidad retorna una matriz con B columnas y K filas de scores. A continuación se muestra el histograma de la fila k=4:

\begin{center}
 \includegraphics[width=0.3\linewidth]{histIris.pdf} 
\end{center}

\par Considerando para cada k la acumulada del histograma,se generaron las siguientes gráficas para cada dataset:

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumGaussianas.pdf} 
        \caption{Dataset 4-Gaus., estabilidad con K-means} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{acumGaussHC.pdf} 
        \caption{Dataset 4-Gaus., estabilidad con Hclust-CL} \label{fig:timing2}
    \end{subfigure}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumIris.pdf} 
        \caption{Dataset Iris, estabilidad con K-means} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{acumIrisHC.pdf} 
        \caption{Dataset Iris, estabilidad con Hclust-CL} \label{fig:timing2}
    \end{subfigure}
        \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumLampone.pdf} 
        \caption{Dataset Lampone, estabilidad con K-means} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{acumLampHC.pdf} 
        \caption{Dataset Lampone, estabilidad con Hclust-CL} \label{fig:timing2}
    \end{subfigure}
    
\end{figure}


\begin{itemize}
\item En el dataset 4-Gaussianas, las soluciones estables son 1 2 y 3. El mayor valor estable, 3, sería entonces razonable de elegir.

\item En el dataset Iris vemos que las soluciones estables son 3 2 y 1 con k-means; por lo que un valor de k razonable a elegir sería el mayor -3-. Sin embargo para h-clust el único valor estable es 1.

\item En el dataset lampone no está muy claro cuáles valores son estables o no, excepto el 1. 

\end{itemize}

\newpage
\section{Ejercicio 4: Opcional}
Analizaré el dataset wine\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Wine}}, relacionado al uso de análisis químicos para identificar clases de vinos. En concreto, este dataset consiste en 13 features numéricas que representan distintas propiedades químicas de una muestra de vino, y una feature 'clase' que varía entre tres tipos de vinos. 178 muestras fueron tomadas. \\

\begin{verbatim}
> summary(wine)
class          alcohol        malic acid         ash                 
 Min.   :1.000   Min.   :11.03   Min.   :0.740   Min.   :1.360     
 1st Qu.:1.000   1st Qu.:12.36   1st Qu.:1.603   1st Qu.:2.210   
 Median :2.000   Median :13.05   Median :1.865   Median :2.360     
 Mean   :1.938   Mean   :13.00   Mean   :2.336   Mean   :2.367   
 3rd Qu.:3.000   3rd Qu.:13.68   3rd Qu.:3.083   3rd Qu.:2.558    
 Max.   :3.000   Max.   :14.83   Max.   :5.800   Max.   :3.230
   
alcalinity of ash   magnesium     total phenols     flavanoids
Min.   :10.60     Min.   : 70.00  Min.   :0.980   Min.   :0.340   
1st Qu.:17.20     1st Qu.: 88.00  1st Qu.:1.742   1st Qu.:1.205   
Median :19.50     Median : 98.00  Median :2.355   Median :2.135   
Mean   :19.49     Mean   : 99.74  Mean   :2.295   Mean   :2.029   
3rd Qu.:21.50     3rd Qu.:107.00  3rd Qu.:2.800   3rd Qu.:2.875    
Max.   :30.00     Max.   :162.00  Max.   :3.880   Max.   :5.080 
  
nonflavanoid phenols proanthocyanins color intensity       hue        
Min.   :0.1300       Min.   :0.410   Min.   : 1.280   Min.   :0.4800  
1st Qu.:0.2700       1st Qu.:1.250   1st Qu.: 3.220   1st Qu.:0.7825  
Median :0.3400       Median :1.555   Median : 4.690   Median :0.9650  
Mean   :0.3619       Mean   :1.591   Mean   : 5.058   Mean   :0.9574  
3rd Qu.:0.4375       3rd Qu.:1.950   3rd Qu.: 6.200   3rd Qu.:1.1200  
Max.   :0.6600       Max.   :3.580   Max.   :13.000   Max.   :1.7100 
 
OD280 of diluted wines    proline      
Min.   :1.270          Min.   : 278.0  
1st Qu.:1.938          1st Qu.: 500.5  
Median :2.780          Median : 673.5  
Mean   :2.612          Mean   : 746.9  
3rd Qu.:3.170          3rd Qu.: 985.0  
Max.   :4.000          Max.   :1680.0 
\end{verbatim}

\par Analizaré este dataset con las técnicas estudiadas para ver si es posible recuperar la clase de vino, y luego veré cuál es el número de clusters real según los métodos vistos. Los análisis se aplicarán sobre las siguientes 5 variantes del dataset:


\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{wineRaw.pdf} 
        \caption{Raw dataset. Algunas features.} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{swine.pdf} 
        \caption{Log + scale} \label{fig:timing2}
    \end{subfigure}   

    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plwine.pdf} 
        \caption{Log + PCA} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{spwine.pdf} 
        \caption{Log + Scale + PCA} \label{fig:timing2}
    \end{subfigure}
    \end{figure}

\begin{figure}
        \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pswine.pdf} 
        \caption{Log + PCA + Scale} \label{fig:timing2}
    \end{subfigure}
\end{figure}

\newpage
Resultados:

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
                    & K-means         & Hclust single & Hclust complete & Hclust average \\  \hline \hline 
Raw                 &  70.22         &   42.7       & 67.42          &      61.24     \\ \hline
Log + scale         &  91.57          &  38.2         & 64.04          &      64.04     \\ \hline
Log + scale + PCA   &  95.51          &   38.76         & 61.8          &      38.76     \\ \hline
Log + PCA           &  72.47           &  38.76         & 58.43          &      35.39     \\ \hline
Log + PCA + Scale   &  96.63          &   38.76        & 61.8          &      62.22     \\ \hline
\end{tabular}
\caption{Porcentajes de casos macheados en pares, dataset wine.}
\end{center}
\end{table}

\par El problema se resuelve muy bien aplicando k-means, y la clasificación mejora mucho escalando los datos. Esto tiene sentido porque hay muchas variables numéricas y las escalas son hasta dos ordenes de magnitud distintas. H-clust complete es el mejor método jerárquico en cuanto a resultado, aunque no tiene mucho sentido aplicarlo porque no estamos buscando jerarquías. \\

\par Respecto al número de clusters óptimo, gapStatistic arroja los siguientes números:

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
                              & Kmeans  & HclA  & HclC \\ \hline \hline 
Wine raw                      &  1      &   1  &  1          \\ \hline
Wine + log + pca              &  3      &   1  &  1          \\ \hline
Wine + log + scale            &  3      &   1  &  1          \\ \hline
Wine + log + scale + pca      &  1      &   1  &  1(2)       \\ \hline
Wine + log + pca + scale      &  3      &   1  &  1          \\ \hline

\end{tabular}
\caption{Número óptimo de clusters para cada problema según el metodo GapStatistic. Moda de 10 ejecuciones.}
\end{center}
\end{table}
\newpage
Nuevamente, k-means recupera el resultado correcto con gap-statistic. Observando los ploteos, vemos que los tres clusters son bastante compactos y diferenciados; por lo tanto tiene sentido que un método sencillo como k-means funcione bien. \\

Para el análisis de estabilidad, veamos la gráfica de las acumuladas:

\begin{figure}[h]
        \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumwinessp.pdf} 
        \caption{Estabilidad. Método k-means.} \label{fig:timing2}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumssphc.pdf} 
        \caption{Estabilidad. Método H-Clust comp.} \label{fig:timing2}
    \end{subfigure}
    
    
\end{figure}
Las soluciones más estables son 1, 2 y 3. Y eligiendo la mayor, el número de clusters indicado sería 3 (guiándonos por k-means, que el más indicado para este problema).
\end{document}
